{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"font-family:verdana;font-size:300%;text-align:center;background-color:#f2f2f2;color:#0d0d0d\">AMMI NLP - Part 1</h1>\n",
    "\n",
    "<h1 style=\"font-family:verdana;font-size:180%;text-align:Center;color:#993333\"> Lab 1: Introduction to text classification  </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:left;background-color:#f2f2f2;color:blue\">Section 1: Text Classification with Naive Bayes Classifier </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this part you'll implement Naive Bayes classifier to classify the text. You need to build a model that predicts the langauge of the text given the words of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, sys, math, re\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    '''\n",
    "    Parameters:\n",
    "    filename (string): path to file to be read\n",
    "    \n",
    "    Return: \n",
    "    List of tuples (explained in first question)\n",
    "    '''\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    data = []\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        data.append((tokens[0], tokens[1:]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('__label__deu',\n",
       " ['Ich', 'würde', 'alles', 'tun,', 'um', 'dich', 'zu', 'beschützen.'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(\"train1.txt\")\n",
    "data[0]\n",
    "# Tuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(data):\n",
    "    '''\n",
    "    Parameters:\n",
    "    \n",
    "    data is  list of [(label, words), (label, worlds), ......]\n",
    "    list of tuples in the shape (string, [list of strings]) )\n",
    "    \n",
    "    Returns: \n",
    "    \n",
    "    This function should return a dictionary containing the following:\n",
    "    { \n",
    "    # label_counts (python dictionary): \n",
    "         {label:  no. of times the label appeared },\n",
    "    # word_counts  (dictionary of dictionaries): \n",
    "         {label: {word: no. of times this word appeared with this label }},\n",
    "    # label_total (int): \n",
    "        total number of labels. (size of train data),\n",
    "    # word_total  (python dictionary) total number of words (from the entire corupus) of the particular label:\n",
    "          {label: no.of words}\n",
    "          \n",
    "          }\n",
    "    \n",
    "    '''\n",
    "    label_total = 0\n",
    "    word_total = defaultdict(lambda: 0)\n",
    "    label_counts = defaultdict(lambda: 0)\n",
    "    word_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "\n",
    "    for example in data:\n",
    "        label, sentence = example\n",
    "        ## FILL CODE\n",
    "\n",
    "\n",
    "    return {'label_counts': label_counts, \n",
    "            'word_counts': word_counts, \n",
    "            'label_total': label_total, \n",
    "            'word_total': word_total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, mu, label_counts, word_counts, label_total, word_total):\n",
    "    '''\n",
    "     Parameters: \n",
    "        sentence (string): sentence to be classified\n",
    "        mu (positive real number): Laplace Smoothing hyperparameter\n",
    "        ** The other parameters introduced in the count_words function\n",
    "    \n",
    "    Returns:\n",
    "    best_label (string): the label that has the highest score. \n",
    "    \n",
    "    Implement the function to predict the best label for the given sentence using Naive Bayes algorithm \n",
    "    \n",
    "    '''\n",
    "    best_label = None\n",
    "    best_score = float('-inf')\n",
    "\n",
    "    for label in word_counts.keys():\n",
    "        score = 0.0\n",
    "        ## FILL CODE\n",
    "\n",
    "    return best_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(valid_data, mu, counts):\n",
    "    '''\n",
    "    Parameters:\n",
    "    valid_data (list of tuples): returned value of load_data function \n",
    "    mu (positive real): Laplace smoothing hyper-parameter\n",
    "    counts (dictionary of dictionaries): return value of count_words_function\n",
    "    \n",
    "    Returns: \n",
    "    accuracy (float): the accuracy of the Naive Bayes classifier\n",
    "    '''\n",
    "    accuracy = 0.0\n",
    "    for label, sentence in valid_data:\n",
    "         ## FILL CODE\n",
    "            pass \n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Naive Bayes **\n",
      "\n",
      "Validation accuracy: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"** Naive Bayes **\")\n",
    "print(\"\")\n",
    "\n",
    "mu = 1.0\n",
    "train_data = load_data(\"train1.txt\")\n",
    "valid_data = load_data(\"valid1.txt\")\n",
    "counts = count_words(train_data)\n",
    "\n",
    "print(\"Validation accuracy: %.3f\" % compute_accuracy(valid_data, mu, counts))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:left;color:black\">***************************************************************</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:left;background-color:#f2f2f2;color:blue\">Section 2: Softmax Classification of Text  </h1>\n",
    "\n",
    "##### In this part you'll implement a Softmax Classifier to classify the text (think of it as a 1 layer feedforward neural network). You need to build a model that predicts the langauge of the text given the words of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(filename, threshold=1):\n",
    "    '''\n",
    "    Parameters:\n",
    "    filename (string): path to the data file\n",
    "    \n",
    "    Returns:\n",
    "    word_dic: dictionary maps words to number of times it appeard in the corpus\n",
    "            dic {word: no of times word appears }\n",
    "    label_dic: dictionary maps labels to integers\n",
    "        dic {label: label_id}\n",
    "    '''\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    word_dict, label_dict = {}, {}\n",
    "    counts = defaultdict(lambda: 0)\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        label = tokens[0]\n",
    "\n",
    "        if not label in label_dict:\n",
    "            label_dict[label] = len(label_dict)\n",
    "\n",
    "        for w in tokens[1:]:\n",
    "            counts[w] += 1\n",
    "            \n",
    "    for k, v in counts.items():\n",
    "        if v > threshold:\n",
    "            word_dict[k] = len(word_dict)\n",
    "    return word_dict, label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, word_dict, label_dict):\n",
    "    '''\n",
    "    ## This function converts the text to a list of tuples of \n",
    "    [(label_id, word_representation),...]\n",
    "    \n",
    "    Parameters:\n",
    "    filename (string): path to the file which contains the data\n",
    "    word_dict (python dictionary): returned by build_dict() function above.\n",
    "    label_dict (python dictionary): reutrned by build_dict() function above() \n",
    "    \n",
    "    Returns:\n",
    "    data (list of tuples): \n",
    "    The representation of the data in the form \n",
    "    [(y_0, x_0, .. (y_i, x_i), ... (y_n, x_n))]\n",
    "    where y is the value of the class \n",
    "    x is the representation of the sentence as a word count vector \n",
    "    \n",
    "    '''\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    data = []\n",
    "    dim = len(word_dict)\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        label = tokens[0]\n",
    "\n",
    "        yi = label_dict[label]\n",
    "        xi = np.zeros(dim)\n",
    "        for word in tokens[1:]:\n",
    "            if word in word_dict:\n",
    "                wid = word_dict[word]\n",
    "                xi[wid] += 1.0\n",
    "        data.append((yi, xi))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict, label_dict = build_dict('train1.txt')\n",
    "d = load_data(\"train1.txt\", word_dict, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([1., 1., 1., ..., 0., 0., 0.])),\n",
       " (0, array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " (1, array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " (0, array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " (2, array([0., 0., 0., ..., 0., 0., 0.]))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    This function should apply softmax to vector x\n",
    "    \n",
    "    Parameter:\n",
    "    x (numpy array)\n",
    "    Returns: \n",
    "    softmax(x) (numpy array)\n",
    "    \n",
    "    '''\n",
    "    ## FILL CODE\n",
    "    m = x.max()\n",
    "    y = np.exp(x-m)\n",
    "    s = y/np.sum(y)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Hint) Derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:center;background-color:#f2f2f2;color:#993333; border:2px; border-style:solid; border-color:gray; padding: 1em\"> \n",
    "   Let $x_i$ be the input vector $W$ is the weight vector $m=$no. of labels, $n=$vocab size\n",
    "    $$ {\\bf S} = W × x_i $$  $x_{i} \\in R^{nx1},  W \\in R^{m×n}$\n",
    "    $$  $$\n",
    "    $${\\bf O} = softmax(s) $$\n",
    "    $${\\bf L} = -log(O[y_{i}]) $$\n",
    "    $$  $$\n",
    "    $$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial S} . \\frac{\\partial S}{\\partial W} $$\n",
    "    $$  $$\n",
    "    $ \\nabla L_{W} = (O-y_{true})$  x   $x_{i}^{T} $  \n",
    "    $$ (O-y_{true}) \\in R^{mx1}, x_{i}  \\in R^{nx1}$$\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, data, niter):\n",
    "    '''\n",
    "    This function should perform the Stochastic Gradient Descent algorithm \n",
    "    \n",
    "    Parameter:\n",
    "    w (numpy array): weight vector\n",
    "    data (list of tuples): [...(y_i, x_i)...] from above\n",
    "    niter (int): number of iterations\n",
    "    \n",
    "    Retunrs:\n",
    "    w (numpy array): weight vector after training\n",
    "    '''\n",
    "    nlabels, dim = w.shape\n",
    "    for iter in range(niter):\n",
    "        ## FILL CODE\n",
    "        train_loss = 0.0\n",
    "        for yi, xi in data:\n",
    "            o = softmax(np.dot(w, xi))\n",
    "            loss = -np.log(o[yi])/len(o)\n",
    "            train_loss += loss\n",
    "            \n",
    "            target = np.zeros(nlabels)\n",
    "            target[yi] = 1.0\n",
    "            \n",
    "            grad = (o - target).reshape(nlabels,1) * xi.reshape(1,-1)\n",
    "            \n",
    "            w = w - .5 * grad\n",
    "            \n",
    "        print(\"iter \"+ str(iter)+\" loss :\" + str(train_loss))\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, x):\n",
    "    '''\n",
    "    This function should compute and return the prediction. \n",
    "    Parameters:\n",
    "    w (numpy array): trained weight vector\n",
    "    x (numpy array): word count vector\n",
    "    \n",
    "    Returns: \n",
    "    prediction (int): index of the correct prediction (y_i)\n",
    "    '''\n",
    "    prediction = np.argmax(softmax(np.dot(w, x)))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(w, valid_data):\n",
    "    '''\n",
    "    This function should compute the accuracy of the classifier \n",
    "    Parameters:\n",
    "    w (numpy array): trained weight vector\n",
    "    valid_data (list of tuples): loaded validation data using load_data() function \n",
    "    \n",
    "    Returns: \n",
    "    accuracy (float): accuracy of the classifier \n",
    "    '''\n",
    "    ## FILL CODE\n",
    "    accuracy = 0.0\n",
    "    for yi, xi in valid_data:\n",
    "        pred = predict(w, xi)\n",
    "        if pred == yi:\n",
    "            accuracy += 1.0\n",
    "            \n",
    "    return accuracy/len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Logistic Regression **\n",
      "\n",
      "iter 0 loss :490.5658282059225\n",
      "iter 1 loss :205.2041208378651\n",
      "iter 2 loss :155.72712964571372\n",
      "iter 3 loss :131.3462632528007\n",
      "iter 4 loss :116.65517172819246\n",
      "\n",
      "Validation accuracy: 0.933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"** Logistic Regression **\")\n",
    "print(\"\")\n",
    "\n",
    "word_dict, label_dict = build_dict(\"train1.txt\")\n",
    "train_data = load_data(\"train1.txt\", word_dict, label_dict)\n",
    "valid_data = load_data(\"valid1.txt\", word_dict, label_dict)\n",
    "\n",
    "nlabels = len(label_dict)\n",
    "dim = len(word_dict)\n",
    "w = np.zeros([nlabels, dim])\n",
    "w = sgd(w, train_data, 5)\n",
    "print(\"\")\n",
    "print(\"Validation accuracy: %.3f\" % compute_accuracy(w, valid_data))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
